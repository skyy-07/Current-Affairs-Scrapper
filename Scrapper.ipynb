{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMGPdfkohMzz90q057Pcnw6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Advanced Current Affairs Web Scraper for Google Colab\n","# Optimized for comprehensive news aggregation from global sources\n","\n","# ============================================================================\n","# INSTALLATION AND IMPORTS\n","# ============================================================================\n","\n","# Install required libraries\n","!pip install requests beautifulsoup4 lxml[html_clean] pandas selenium newspaper3k feedparser aiohttp asyncio python-dateutil textstat\n","\n","# For Selenium in Colab\n","!apt update\n","!apt install chromium-chromedriver\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import json\n","import time\n","import asyncio\n","import aiohttp\n","from datetime import datetime, timedelta\n","from urllib.parse import urljoin, urlparse\n","import re\n","import feedparser\n","import newspaper\n","from newspaper import Article\n","from selenium import webdriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.chrome.options import Options\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# ============================================================================\n","# COMPREHENSIVE NEWS SOURCES DATABASE\n","# ============================================================================\n","\n","NEWS_SOURCES = {\n","    \"global_english\": {\n","        \"BBC\": {\n","            \"rss\": \"https://feeds.bbci.co.uk/news/rss.xml\",\n","            \"world\": \"https://feeds.bbci.co.uk/news/world/rss.xml\",\n","            \"politics\": \"https://feeds.bbci.co.uk/news/politics/rss.xml\",\n","            \"business\": \"https://feeds.bbci.co.uk/news/business/rss.xml\"\n","        },\n","        \"CNN\": {\n","            \"top\": \"http://rss.cnn.com/rss/edition.rss\",\n","            \"world\": \"http://rss.cnn.com/rss/edition_world.rss\",\n","            \"politics\": \"http://rss.cnn.com/rss/edition_politics.rss\",\n","            \"business\": \"http://rss.cnn.com/rss/money_latest.rss\"\n","        },\n","        \"Reuters\": {\n","            \"world\": \"http://feeds.reuters.com/Reuters/worldNews\",\n","            \"politics\": \"http://feeds.reuters.com/Reuters/PoliticsNews\",\n","            \"business\": \"http://feeds.reuters.com/reuters/businessNews\",\n","            \"top\": \"http://feeds.reuters.com/reuters/topNews\"\n","        },\n","        \"Guardian\": {\n","            \"world\": \"https://www.theguardian.com/world/rss\",\n","            \"politics\": \"https://www.theguardian.com/politics/rss\",\n","            \"business\": \"https://www.theguardian.com/business/rss\"\n","        },\n","        \"Al Jazeera\": {\n","            \"main\": \"https://www.aljazeera.com/xml/rss/all.xml\"\n","        },\n","        \"Associated Press\": {\n","            \"top\": \"https://apnews.com/rss\"\n","        },\n","        \"NPR\": {\n","            \"news\": \"https://feeds.npr.org/1001/rss.xml\"\n","        }\n","    },\n","    \"indian_english\": {\n","        \"Times of India\": {\n","            \"top\": \"https://timesofindia.indiatimes.com/rssfeedstopstories.cms\",\n","            \"india\": \"https://timesofindia.indiatimes.com/rssfeeds/296589292.cms\",\n","            \"world\": \"https://timesofindia.indiatimes.com/rssfeeds/296589292.cms\",\n","            \"business\": \"https://timesofindia.indiatimes.com/rssfeeds/1898055.cms\"\n","        },\n","        \"Hindu\": {\n","            \"national\": \"https://www.thehindu.com/news/national/?service=rss\",\n","            \"international\": \"https://www.thehindu.com/news/international/?service=rss\",\n","            \"business\": \"https://www.thehindu.com/business/?service=rss\"\n","        },\n","        \"Indian Express\": {\n","            \"main\": \"https://indianexpress.com/feed/\"\n","        },\n","        \"NDTV\": {\n","            \"main\": \"https://feeds.feedburner.com/NDTV-LatestNews\"\n","        },\n","        \"Firstpost\": {\n","            \"main\": \"https://www.firstpost.com/commonfeeds/v1/eng/rss/India.xml\"\n","        }\n","    },\n","    \"us_sources\": {\n","        \"NY Times\": {\n","            \"home\": \"https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml\",\n","            \"world\": \"https://rss.nytimes.com/services/xml/rss/nyt/World.xml\",\n","            \"politics\": \"https://rss.nytimes.com/services/xml/rss/nyt/Politics.xml\"\n","        },\n","        \"Washington Post\": {\n","            \"politics\": \"http://feeds.washingtonpost.com/rss/politics\"\n","        },\n","        \"Politico\": {\n","            \"main\": \"https://www.politico.com/rss/politics08.xml\"\n","        }\n","    },\n","    \"regional\": {\n","        \"France24\": {\n","            \"main\": \"https://www.france24.com/en/rss\"\n","        },\n","        \"DW\": {\n","            \"main\": \"https://rss.dw.com/rdf/rss-en-all\"\n","        }\n","    }\n","}\n","\n","# Current affairs keywords for filtering relevant content\n","CURRENT_AFFAIRS_KEYWORDS = [\n","    'politics', 'government', 'election', 'policy', 'parliament', 'congress', 'senate',\n","    'minister', 'president', 'prime minister', 'cabinet', 'legislation', 'law',\n","    'economy', 'gdp', 'inflation', 'budget', 'tax', 'finance', 'market', 'trade',\n","    'international', 'diplomacy', 'foreign policy', 'summit', 'treaty', 'war',\n","    'conflict', 'crisis', 'protest', 'strike', 'reform', 'scandal', 'investigation',\n","    'court', 'justice', 'ruling', 'verdict', 'supreme court', 'high court',\n","    'social', 'education', 'healthcare', 'environment', 'climate', 'energy'\n","]\n","\n","# ============================================================================\n","# ADVANCED CURRENT AFFAIRS SCRAPER CLASS\n","# ============================================================================\n","\n","class CurrentAffairsScraper:\n","    \"\"\"\n","    Advanced scraper optimized for comprehensive current affairs collection\n","    from global news sources with intelligent filtering and text processing.\n","    \"\"\"\n","\n","    def __init__(self, max_articles_per_source=20, days_back=7):\n","        self.session = requests.Session()\n","        self.session.headers.update({\n","            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n","        })\n","        self.max_articles_per_source = max_articles_per_source\n","        self.days_back = days_back\n","        self.cutoff_date = datetime.now() - timedelta(days=days_back)\n","        self.all_articles = []\n","        self.processed_articles = []\n","\n","    def is_current_affairs_relevant(self, title, description=\"\", content=\"\"):\n","        \"\"\"Check if article is relevant to current affairs using keyword matching.\"\"\"\n","        text_to_check = f\"{title} {description} {content}\".lower()\n","\n","        # Count keyword matches\n","        keyword_matches = sum(1 for keyword in CURRENT_AFFAIRS_KEYWORDS if keyword in text_to_check)\n","\n","        # Consider relevant if has multiple keyword matches or specific high-value keywords\n","        high_value_keywords = ['government', 'politics', 'election', 'policy', 'parliament', 'minister', 'president']\n","        has_high_value = any(keyword in text_to_check for keyword in high_value_keywords)\n","\n","        return keyword_matches >= 2 or has_high_value\n","\n","    def parse_rss_feed(self, rss_url, source_name, category=\"general\"):\n","        \"\"\"Parse RSS feed and extract current affairs articles.\"\"\"\n","        articles = []\n","        try:\n","            feed = feedparser.parse(rss_url)\n","\n","            for entry in feed.entries[:self.max_articles_per_source]:\n","                try:\n","                    # Parse publication date\n","                    pub_date = None\n","                    if hasattr(entry, 'published_parsed') and entry.published_parsed:\n","                        pub_date = datetime(*entry.published_parsed[:6])\n","                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:\n","                        pub_date = datetime(*entry.updated_parsed[:6])\n","\n","                    # Skip old articles\n","                    if pub_date and pub_date < self.cutoff_date:\n","                        continue\n","\n","                    title = entry.get('title', '')\n","                    description = entry.get('description', '') or entry.get('summary', '')\n","                    link = entry.get('link', '')\n","\n","                    # Check if relevant to current affairs\n","                    if self.is_current_affairs_relevant(title, description):\n","                        article = {\n","                            'source': source_name,\n","                            'category': category,\n","                            'title': title,\n","                            'description': BeautifulSoup(description, 'html.parser').get_text(),\n","                            'link': link,\n","                            'published': pub_date.isoformat() if pub_date else datetime.now().isoformat(),\n","                            'scraped_at': datetime.now().isoformat()\n","                        }\n","                        articles.append(article)\n","\n","                except Exception as e:\n","                    print(f\"Error parsing entry from {source_name}: {e}\")\n","                    continue\n","\n","        except Exception as e:\n","            print(f\"Error parsing RSS feed {rss_url}: {e}\")\n","\n","        return articles\n","\n","    def extract_full_article_content(self, url):\n","        \"\"\"Extract full article content using newspaper3k.\"\"\"\n","        try:\n","            article = Article(url)\n","            article.download()\n","            article.parse()\n","\n","            return {\n","                'full_text': article.text,\n","                'authors': article.authors,\n","                'keywords': article.keywords,\n","                'summary': article.summary if hasattr(article, 'summary') else ''\n","            }\n","        except Exception as e:\n","            print(f\"Error extracting content from {url}: {e}\")\n","            return {}\n","\n","    async def scrape_source_async(self, source_data, source_name):\n","        \"\"\"Asynchronously scrape a single news source.\"\"\"\n","        source_articles = []\n","\n","        for category, rss_url in source_data.items():\n","            articles = self.parse_rss_feed(rss_url, source_name, category)\n","            source_articles.extend(articles)\n","\n","        return source_articles\n","\n","    def scrape_all_sources(self):\n","        \"\"\"Scrape all configured news sources for current affairs.\"\"\"\n","        print(\"🚀 Starting comprehensive current affairs scraping...\")\n","        start_time = time.time()\n","\n","        all_articles = []\n","        total_sources = sum(len(sources) for region_sources in NEWS_SOURCES.values()\n","                          for sources in region_sources.values())\n","        processed_sources = 0\n","\n","        # Process each region\n","        for region, sources in NEWS_SOURCES.items():\n","            print(f\"\\n📰 Scraping {region.replace('_', ' ').title()} sources...\")\n","\n","            for source_name, source_data in sources.items():\n","                try:\n","                    print(f\"  → Processing {source_name}...\")\n","\n","                    source_articles = []\n","                    for category, rss_url in source_data.items():\n","                        articles = self.parse_rss_feed(rss_url, source_name, category)\n","                        source_articles.extend(articles)\n","\n","                    all_articles.extend(source_articles)\n","                    processed_sources += 1\n","\n","                    print(f\"    ✓ Found {len(source_articles)} current affairs articles\")\n","\n","                    # Add delay to be respectful\n","                    time.sleep(0.5)\n","\n","                except Exception as e:\n","                    print(f\"    ✗ Error processing {source_name}: {e}\")\n","                    continue\n","\n","        self.all_articles = all_articles\n","\n","        elapsed_time = time.time() - start_time\n","        print(f\"\\n✅ Scraping completed in {elapsed_time:.2f} seconds\")\n","        print(f\"📊 Total articles collected: {len(all_articles)}\")\n","        print(f\"📈 Sources processed: {processed_sources}/{total_sources}\")\n","\n","        return all_articles\n","\n","    def deduplicate_articles(self):\n","        \"\"\"Remove duplicate articles based on title similarity.\"\"\"\n","        unique_articles = []\n","        seen_titles = set()\n","\n","        for article in self.all_articles:\n","            # Create a normalized title for comparison\n","            normalized_title = re.sub(r'[^a-zA-Z0-9\\s]', '', article['title'].lower())\n","            normalized_title = ' '.join(normalized_title.split())\n","\n","            # Check for duplicates\n","            is_duplicate = False\n","            for seen_title in seen_titles:\n","                # Simple similarity check - if 80% of words match\n","                title_words = set(normalized_title.split())\n","                seen_words = set(seen_title.split())\n","\n","                if len(title_words) > 0 and len(seen_words) > 0:\n","                    similarity = len(title_words & seen_words) / len(title_words | seen_words)\n","                    if similarity > 0.8:\n","                        is_duplicate = True\n","                        break\n","\n","            if not is_duplicate:\n","                unique_articles.append(article)\n","                seen_titles.add(normalized_title)\n","\n","        print(f\"🔄 Removed {len(self.all_articles) - len(unique_articles)} duplicate articles\")\n","        self.all_articles = unique_articles\n","        return unique_articles\n","\n","    def categorize_articles(self):\n","        \"\"\"Categorize articles by topic area.\"\"\"\n","        categories = {\n","            'politics': ['politic', 'government', 'parliament', 'election', 'minister', 'cabinet'],\n","            'economy': ['economy', 'economic', 'gdp', 'inflation', 'budget', 'finance', 'market'],\n","            'international': ['international', 'foreign', 'diplomacy', 'summit', 'treaty', 'global'],\n","            'social': ['social', 'education', 'healthcare', 'society', 'culture'],\n","            'legal': ['court', 'justice', 'legal', 'law', 'ruling', 'verdict'],\n","            'environment': ['environment', 'climate', 'energy', 'pollution', 'green']\n","        }\n","\n","        for article in self.all_articles:\n","            text_to_analyze = f\"{article['title']} {article['description']}\".lower()\n","\n","            # Assign primary category based on keyword matches\n","            max_matches = 0\n","            primary_category = 'general'\n","\n","            for category, keywords in categories.items():\n","                matches = sum(1 for keyword in keywords if keyword in text_to_analyze)\n","                if matches > max_matches:\n","                    max_matches = matches\n","                    primary_category = category\n","\n","            article['topic_category'] = primary_category\n","\n","    def generate_summary_text(self):\n","        \"\"\"Generate a comprehensive text summary of all current affairs.\"\"\"\n","        if not self.all_articles:\n","            return \"No current affairs articles found.\"\n","\n","        # Sort articles by date (most recent first)\n","        sorted_articles = sorted(self.all_articles,\n","                               key=lambda x: x['published'], reverse=True)\n","\n","        # Generate summary text\n","        summary_parts = []\n","        summary_parts.append(f\"COMPREHENSIVE CURRENT AFFAIRS SUMMARY\")\n","        summary_parts.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","        summary_parts.append(f\"Period covered: Last {self.days_back} days\")\n","        summary_parts.append(f\"Total articles: {len(sorted_articles)}\")\n","        summary_parts.append(\"=\" * 80)\n","\n","        # Group by category\n","        categories = {}\n","        for article in sorted_articles:\n","            category = article.get('topic_category', 'general')\n","            if category not in categories:\n","                categories[category] = []\n","            categories[category].append(article)\n","\n","        # Generate category-wise summaries\n","        for category, articles in categories.items():\n","            if not articles:\n","                continue\n","\n","            summary_parts.append(f\"\\n{category.upper()} ({len(articles)} articles)\")\n","            summary_parts.append(\"-\" * 50)\n","\n","            for i, article in enumerate(articles[:10], 1):  # Top 10 per category\n","                summary_parts.append(f\"{i}. {article['title']}\")\n","                summary_parts.append(f\"   Source: {article['source']} | Published: {article['published'][:10]}\")\n","                if article['description']:\n","                    description = article['description'][:200] + \"...\" if len(article['description']) > 200 else article['description']\n","                    summary_parts.append(f\"   {description}\")\n","                summary_parts.append(f\"   URL: {article['link']}\")\n","                summary_parts.append(\"\")\n","\n","        # Add source statistics\n","        summary_parts.append(\"\\nSOURCE STATISTICS\")\n","        summary_parts.append(\"-\" * 50)\n","        source_counts = {}\n","        for article in sorted_articles:\n","            source = article['source']\n","            source_counts[source] = source_counts.get(source, 0) + 1\n","\n","        for source, count in sorted(source_counts.items(), key=lambda x: x[1], reverse=True):\n","            summary_parts.append(f\"{source}: {count} articles\")\n","\n","        return \"\\n\".join(summary_parts)\n","\n","    def save_results(self, output_format='text'):\n","        \"\"\"Save results in specified format.\"\"\"\n","        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","\n","        if output_format == 'text':\n","            filename = f'current_affairs_{timestamp}.txt'\n","            with open(filename, 'w', encoding='utf-8') as f:\n","                f.write(self.generate_summary_text())\n","            print(f\"📄 Text summary saved to: {filename}\")\n","\n","        elif output_format == 'json':\n","            filename = f'current_affairs_{timestamp}.json'\n","            with open(filename, 'w', encoding='utf-8') as f:\n","                json.dump(self.all_articles, f, ensure_ascii=False, indent=2)\n","            print(f\"📄 JSON data saved to: {filename}\")\n","\n","        elif output_format == 'csv':\n","            filename = f'current_affairs_{timestamp}.csv'\n","            df = pd.DataFrame(self.all_articles)\n","            df.to_csv(filename, index=False, encoding='utf-8')\n","            print(f\"📄 CSV data saved to: {filename}\")\n","\n","        return filename\n","\n","# ============================================================================\n","# MAIN EXECUTION FUNCTIONS\n","# ============================================================================\n","\n","def scrape_comprehensive_current_affairs(days_back=7, max_articles_per_source=15):\n","    \"\"\"\n","    Main function to scrape comprehensive current affairs from all sources.\n","\n","    Args:\n","        days_back (int): Number of days to look back for articles\n","        max_articles_per_source (int): Maximum articles to collect per RSS feed\n","\n","    Returns:\n","        str: Formatted text summary of current affairs\n","    \"\"\"\n","    print(\"🌍 COMPREHENSIVE CURRENT AFFAIRS SCRAPER\")\n","    print(\"=\" * 50)\n","\n","    # Initialize scraper\n","    scraper = CurrentAffairsScraper(max_articles_per_source, days_back)\n","\n","    # Scrape all sources\n","    articles = scraper.scrape_all_sources()\n","\n","    if not articles:\n","        return \"No current affairs articles found. Please check your internet connection and try again.\"\n","\n","    # Process articles\n","    print(\"\\n🔄 Processing articles...\")\n","    scraper.deduplicate_articles()\n","    scraper.categorize_articles()\n","\n","    # Generate and save summary\n","    summary_text = scraper.generate_summary_text()\n","\n","    # Save in multiple formats\n","    scraper.save_results('text')\n","    scraper.save_results('json')\n","    scraper.save_results('csv')\n","\n","    return summary_text\n","\n","def quick_current_affairs_update():\n","    \"\"\"Quick function for immediate current affairs update.\"\"\"\n","    return scrape_comprehensive_current_affairs(days_back=3, max_articles_per_source=10)\n","\n","def deep_current_affairs_analysis():\n","    \"\"\"Comprehensive function for detailed current affairs analysis.\"\"\"\n","    return scrape_comprehensive_current_affairs(days_back=14, max_articles_per_source=25)\n","\n","# ============================================================================\n","# EXAMPLE USAGE\n","# ============================================================================\n","\n","if __name__ == \"__main__\":\n","    print(\"🚀 Starting Comprehensive Current Affairs Scraping...\")\n","\n","    # Get comprehensive current affairs summary\n","    summary = scrape_comprehensive_current_affairs()\n","\n","    # Display first part of summary\n","    print(\"\\n\" + \"=\"*80)\n","    print(summary[:2000] + \"\\n[... Summary continues in saved files ...]\")\n","    print(\"=\"*80)\n","\n","# ============================================================================\n","# QUICK START EXAMPLES FOR COLAB\n","# ============================================================================\n","\n","\"\"\"\n","QUICK START GUIDE FOR CURRENT AFFAIRS SCRAPING:\n","\n","1. BASIC USAGE:\n","   summary = scrape_comprehensive_current_affairs()\n","   print(summary)\n","\n","2. QUICK UPDATE (Last 3 days):\n","   summary = quick_current_affairs_update()\n","\n","3. DEEP ANALYSIS (Last 14 days):\n","   summary = scrape_comprehensive_current_affairs(days_back=14)\n","\n","4. CUSTOM PARAMETERS:\n","   summary = scrape_comprehensive_current_affairs(days_back=5, max_articles_per_source=20)\n","\n","5. ACCESS RAW DATA:\n","   scraper = CurrentAffairsScraper()\n","   articles = scraper.scrape_all_sources()\n","   scraper.save_results('json')  # Save as JSON\n","   scraper.save_results('csv')   # Save as CSV\n","\n","FEATURES:\n","✓ 25+ Global news sources (BBC, CNN, Reuters, Times of India, etc.)\n","✓ Intelligent current affairs filtering\n","✓ Automatic categorization (Politics, Economy, International, etc.)\n","✓ Duplicate removal\n","✓ Multiple output formats (Text, JSON, CSV)\n","✓ Date-based filtering\n","✓ Source statistics and analytics\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"KpAsCjSpC8g_","executionInfo":{"status":"ok","timestamp":1759169850957,"user_tz":-330,"elapsed":100878,"user":{"displayName":"Nilavra Sinha","userId":"10049446822855792696"}},"outputId":"fe76edb2-1478-4c12-c966-9d994ec98a3a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Collecting selenium\n","  Downloading selenium-4.35.0-py3-none-any.whl.metadata (7.4 kB)\n","Collecting newspaper3k\n","  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n","Collecting feedparser\n","  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (3.12.15)\n","Collecting asyncio\n","  Downloading asyncio-4.0.0-py3-none-any.whl.metadata (994 bytes)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (2.9.0.post0)\n","Collecting textstat\n","  Downloading textstat-0.7.10-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: lxml[html_clean] in /usr/local/lib/python3.12/dist-packages (5.4.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n","Collecting lxml_html_clean (from lxml[html_clean])\n","  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Collecting trio~=0.30.0 (from selenium)\n","  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n","Collecting trio-websocket~=0.12.2 (from selenium)\n","  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n","Collecting typing-extensions>=4.0.0 (from beautifulsoup4)\n","  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.8.0)\n","Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (11.3.0)\n","Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (6.0.2)\n","Collecting cssselect>=0.9.2 (from newspaper3k)\n","  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (3.9.1)\n","Collecting tldextract>=2.0.1 (from newspaper3k)\n","  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n","Collecting feedfinder2>=0.0.4 (from newspaper3k)\n","  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting jieba3k>=0.35.1 (from newspaper3k)\n","  Downloading jieba3k-0.35.1.zip (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tinysegmenter==0.3 (from newspaper3k)\n","  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting sgmllib3k (from feedparser)\n","  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (6.6.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (1.20.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil) (1.17.0)\n","Collecting pyphen (from textstat)\n","  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from textstat) (75.2.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n","Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n","  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.19.1)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n","Collecting outcome (from trio~=0.30.0->selenium)\n","  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n","Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n","  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n","Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n","Downloading selenium-4.35.0-py3-none-any.whl (9.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading asyncio-4.0.0-py3-none-any.whl (5.6 kB)\n","Downloading textstat-0.7.10-py3-none-any.whl (239 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.2/239.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n","Downloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n","Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n","Downloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n","Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n","Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n","Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n","  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=68414819472f7082416e40d9d3dff6881e10c83de6973f7b2d10fa7b98270180\n","  Stored in directory: /root/.cache/pip/wheels/a5/91/9f/00d66475960891a64867914273fcaf78df6cb04d905b104a2a\n","  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=d95d89149361315ad6e1cd483ddd12f3410b4fdd4b32d0e63ea8bf55ebf8de7d\n","  Stored in directory: /root/.cache/pip/wheels/9f/9f/fb/364871d7426d3cdd4d293dcf7e53d97f160c508b2ccf00cc79\n","  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=1c54a927087224b6b1ef72c00c29b416d159c0b5fde7e8b0e5089efe9372c396\n","  Stored in directory: /root/.cache/pip/wheels/26/72/f7/fff392a8d4ea988dea4ccf9788599d09462a7f5e51e04f8a92\n","  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=3232eaa9dc9fd776bc1f86c143aef0a3352dcb2d047c8c17595fd1f4d3b19153\n","  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n","Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n","Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, wsproto, typing-extensions, pyphen, outcome, lxml_html_clean, feedparser, cssselect, asyncio, trio, textstat, requests-file, trio-websocket, tldextract, feedfinder2, selenium, newspaper3k\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.15.0\n","    Uninstalling typing_extensions-4.15.0:\n","      Successfully uninstalled typing_extensions-4.15.0\n","Successfully installed asyncio-4.0.0 cssselect-1.3.0 feedfinder2-0.0.4 feedparser-6.0.12 jieba3k-0.35.1 lxml_html_clean-0.4.2 newspaper3k-0.2.8 outcome-1.3.0.post0 pyphen-0.17.2 requests-file-2.1.0 selenium-4.35.0 sgmllib3k-1.0.0 textstat-0.7.10 tinysegmenter-0.3 tldextract-5.3.0 trio-0.30.0 trio-websocket-0.12.2 typing-extensions-4.14.1 wsproto-1.2.0\n","Get:1 https://cli.github.com/packages stable InRelease [3,917 B]\n","Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n","Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,014 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,805 kB]\n","Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,690 kB]\n","Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,305 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n","Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,371 kB]\n","Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,274 kB]\n","Fetched 22.9 MB in 3s (8,164 kB/s)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","46 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  apparmor chromium-browser libfuse3-3 libudev1 snapd squashfs-tools\n","  systemd-hwe-hwdb udev\n","Suggested packages:\n","  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n","The following NEW packages will be installed:\n","  apparmor chromium-browser chromium-chromedriver libfuse3-3 snapd\n","  squashfs-tools systemd-hwe-hwdb udev\n","The following packages will be upgraded:\n","  libudev1\n","1 upgraded, 8 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 32.5 MB of archives.\n","After this operation, 130 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.16 [76.7 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.16 [1,557 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.68.5+ubuntu22.04.1 [30.0 MB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.6 [3,668 B]\n","Fetched 32.5 MB in 1s (26.2 MB/s)\n","Preconfiguring packages ...\n","Selecting previously unselected package apparmor.\n","(Reading database ... 126441 files and directories currently installed.)\n","Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n","Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n","Selecting previously unselected package squashfs-tools.\n","Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n","Unpacking squashfs-tools (1:4.5-3build1) ...\n","Preparing to unpack .../libudev1_249.11-0ubuntu3.16_amd64.deb ...\n","Unpacking libudev1:amd64 (249.11-0ubuntu3.16) over (249.11-0ubuntu3.12) ...\n","Setting up libudev1:amd64 (249.11-0ubuntu3.16) ...\n","Selecting previously unselected package udev.\n","(Reading database ... 126641 files and directories currently installed.)\n","Preparing to unpack .../udev_249.11-0ubuntu3.16_amd64.deb ...\n","Unpacking udev (249.11-0ubuntu3.16) ...\n","Selecting previously unselected package libfuse3-3:amd64.\n","Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n","Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n","Selecting previously unselected package snapd.\n","Preparing to unpack .../snapd_2.68.5+ubuntu22.04.1_amd64.deb ...\n","Unpacking snapd (2.68.5+ubuntu22.04.1) ...\n","Setting up apparmor (3.0.4-2ubuntu2.4) ...\n","Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n","Setting up squashfs-tools (1:4.5-3build1) ...\n","Setting up udev (249.11-0ubuntu3.16) ...\n","invoke-rc.d: could not determine current runlevel\n","invoke-rc.d: policy-rc.d denied execution of start.\n","Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n","Setting up snapd (2.68.5+ubuntu22.04.1) ...\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n","Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n","Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n","Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n","Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n","Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n","Selecting previously unselected package chromium-browser.\n","(Reading database ... 126868 files and directories currently installed.)\n","Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n","=> Installing the chromium snap\n","==> Checking connectivity with the snap store\n","===> System doesn't have a working snapd, skipping\n","Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","Selecting previously unselected package chromium-chromedriver.\n","Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n","Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","Selecting previously unselected package systemd-hwe-hwdb.\n","Preparing to unpack .../systemd-hwe-hwdb_249.11.6_all.deb ...\n","Unpacking systemd-hwe-hwdb (249.11.6) ...\n","Setting up systemd-hwe-hwdb (249.11.6) ...\n","Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n","Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","Processing triggers for udev (249.11-0ubuntu3.16) ...\n","Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n","🚀 Starting Comprehensive Current Affairs Scraping...\n","🌍 COMPREHENSIVE CURRENT AFFAIRS SCRAPER\n","==================================================\n","🚀 Starting comprehensive current affairs scraping...\n","\n","📰 Scraping Global English sources...\n","  → Processing BBC...\n","    ✓ Found 17 current affairs articles\n","  → Processing CNN...\n","    ✓ Found 0 current affairs articles\n","  → Processing Reuters...\n","    ✓ Found 0 current affairs articles\n","  → Processing Guardian...\n","    ✓ Found 31 current affairs articles\n","  → Processing Al Jazeera...\n","    ✓ Found 7 current affairs articles\n","  → Processing Associated Press...\n","    ✓ Found 0 current affairs articles\n","  → Processing NPR...\n","    ✓ Found 4 current affairs articles\n","\n","📰 Scraping Indian English sources...\n","  → Processing Times of India...\n","    ✓ Found 20 current affairs articles\n","  → Processing Hindu...\n","    ✓ Found 16 current affairs articles\n","  → Processing Indian Express...\n","    ✓ Found 1 current affairs articles\n","  → Processing NDTV...\n","    ✓ Found 3 current affairs articles\n","  → Processing Firstpost...\n","    ✓ Found 0 current affairs articles\n","\n","📰 Scraping Us Sources sources...\n","  → Processing NY Times...\n","    ✓ Found 20 current affairs articles\n","  → Processing Washington Post...\n","    ✓ Found 8 current affairs articles\n","  → Processing Politico...\n","    ✓ Found 0 current affairs articles\n","\n","📰 Scraping Regional sources...\n","  → Processing France24...\n","    ✓ Found 6 current affairs articles\n","  → Processing DW...\n","    ✓ Found 4 current affairs articles\n","\n","✅ Scraping completed in 34.79 seconds\n","📊 Total articles collected: 137\n","📈 Sources processed: 17/35\n","\n","🔄 Processing articles...\n","🔄 Removed 12 duplicate articles\n","📄 Text summary saved to: current_affairs_20250929_181730.txt\n","📄 JSON data saved to: current_affairs_20250929_181730.json\n","📄 CSV data saved to: current_affairs_20250929_181730.csv\n","\n","================================================================================\n","COMPREHENSIVE CURRENT AFFAIRS SUMMARY\n","Generated on: 2025-09-29 18:17:30\n","Period covered: Last 7 days\n","Total articles: 125\n","================================================================================\n","\n","POLITICS (74 articles)\n","--------------------------------------------------\n","1. Live Updates: Trump and Netanyahu Meeting to Discuss Gaza Plans\n","   Source: NY Times | Published: 2025-09-29\n","   Prime Minister Benjamin Netanyahu arrived at the White House for talks on Gaza’s postwar future and as Israel’s international isolation has deepened. Several European countries recently announced they...\n","   URL: https://www.nytimes.com/live/2025/09/29/world/trump-netanyahu-israel-gaza\n","\n","2. Moldova Moves Toward Europe, but Russian Tug of War Persists\n","   Source: NY Times | Published: 2025-09-29\n","   Moldova’s pro-European party held onto its absolute majority in Parliament in national elections, but it still has a long road into the European Union.\n","   URL: https://www.nytimes.com/2025/09/29/world/europe/moldova-elections-russia-europe.html\n","\n","3. Government appoints S C Murmu as RBI Deputy Governor\n","   Source: Indian Express | Published: 2025-09-29\n","   URL: https://indianexpress.com/article/business/government-appoints-s-c-murmu-as-rbi-deputy-governor-10279050/\n","\n","4. \"Unimaginably Painful\": Nirmala Sitharaman Visits Karur Stampede Site\n","   Source: NDTV | Published: 2025-09-29\n","   \"Such tragedies should never happen again,\" Union Finance Minister Nirmala Sitharaman said on Monday after visiting Karur, Tamil Nadu, where a deadly stampede at actor-politician Vijay's Tamilaga...\n","   URL: https://www.ndtv.com/india-news/unimaginably-painful-nirmala-sitharaman-visits-karur-stampede-site-9367659\n","\n","5. S Jaishankar Meets Canadian Counterpart In New York\n","   Source: NDTV | Published: 2025-09-29\n","   External Affairs Minister S Jaishankar met Canadian Minister of Foreign Affairs Anita Anand here, as he termed the appointment of envoys to Delhi and Ottawa as a \"welcome\" step to rebuild ties between...\n","   URL: http\n","[... Summary continues in saved files ...]\n","================================================================================\n"]},{"output_type":"execute_result","data":{"text/plain":["\"\\nQUICK START GUIDE FOR CURRENT AFFAIRS SCRAPING:\\n\\n1. BASIC USAGE:\\n   summary = scrape_comprehensive_current_affairs()\\n   print(summary)\\n\\n2. QUICK UPDATE (Last 3 days):\\n   summary = quick_current_affairs_update()\\n\\n3. DEEP ANALYSIS (Last 14 days):\\n   summary = scrape_comprehensive_current_affairs(days_back=14)\\n\\n4. CUSTOM PARAMETERS:\\n   summary = scrape_comprehensive_current_affairs(days_back=5, max_articles_per_source=20)\\n\\n5. ACCESS RAW DATA:\\n   scraper = CurrentAffairsScraper()\\n   articles = scraper.scrape_all_sources()\\n   scraper.save_results('json')  # Save as JSON\\n   scraper.save_results('csv')   # Save as CSV\\n\\nFEATURES:\\n✓ 25+ Global news sources (BBC, CNN, Reuters, Times of India, etc.)\\n✓ Intelligent current affairs filtering\\n✓ Automatic categorization (Politics, Economy, International, etc.)\\n✓ Duplicate removal\\n✓ Multiple output formats (Text, JSON, CSV)\\n✓ Date-based filtering\\n✓ Source statistics and analytics\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":[],"metadata":{"id":"zYMllZeSRq2E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"i9lD2ynhTgRE"}},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"mq0MIOfgTics"}},{"cell_type":"code","source":[],"metadata":{"id":"f6hntdHMTebt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ldy4XyAvCoP5"},"execution_count":null,"outputs":[]}]}